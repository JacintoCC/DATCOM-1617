---
title: "Clasificación no balanceada"
author: "Jacinto Carrasco Castillo"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = T)
library(ggplot2)
```

# Preparación de datos y rendimiento básico de clasificación.


```{r load.datasets}
## load the circle dataset
circle <- read.table("Data/circle.txt", sep=",",
                     col.names = c("Att1", "Att2", "Class"),
                     colClasses = c("numeric", "numeric", "factor"))
```


```{r}
# determine the imbalance ratio
nClass0 <- sum(circle$Class == 0)
nClass1 <- sum(circle$Class == 1)
IR <- nClass1 / nClass0
IR
```

   En el conjunto de datos `circle`, representado en la Figura \ref{fig:circle} el ratio es de 42.45 elementos de la clase negativa por cada elemento de la clase positiva.


```{r circle, fig.cap="Distribución para la clase circle"}
# visualize the data distribution
ggplot(circle, aes(x = Att1, y = Att2, col = Class)) + 
   geom_point()
```


```{r knn.base}
# Set up the dataset for 5 fold cross validation.
# Make sure to respect the class imbalance in the folds.
pos <- which(circle$Class==0)
neg <- which(circle$Class==1)

CVperm_pos <- matrix(sample(pos), ncol=5, byrow=T)
CVperm_neg <- matrix(sample(neg), ncol=5, byrow=T)

CVperm <- rbind(CVperm_pos, CVperm_neg)

# Base performance of 3NN
knn.pred = NULL
library(class)
for(i in 1:5){
  predictions <- knn(circle[-CVperm[,i], -3], 
                     circle[CVperm[,i], -3],
                     circle[-CVperm[,i], 3], k = 3)
  knn.pred <- c(knn.pred, predictions)
}
acc <- sum((circle$Class[as.vector(CVperm)] == 0 & knn.pred == 1) 
           | (circle$Class[as.vector(CVperm)] == 1 & knn.pred == 2)) / (nClass0 + nClass1)
tpr <- sum(circle$Class[as.vector(CVperm)] == 0 & knn.pred == 1) / nClass0
tnr <- sum(circle$Class[as.vector(CVperm)] == 1 & knn.pred == 2) / nClass1
gmean <- sqrt(tpr * tnr)
gmean
```

   Para el conjunto de datos `circle` partimos de una medida G de la calidad del clasificador en torno a  $0.88$. A continuación veremos si esta medida aumenta con técnicas de sobremuestreo y bajomuestreo.

# Random Oversampling (ROS)

```{r}
knn.pred = NULL
for( i in 1:5){
  train <- circle[-CVperm[,i], -3]
  classes.train <- circle[-CVperm[,i], 3] 
  test  <- circle[CVperm[,i], -3]
  
  # Sobremuestreamos de forma aleatoria la clase minoriatia, la 0.
  
  minority.indices <- which(classes.train == 0)
  # Sacamos la cantidad de ejemplos que tenemos que añadir.
  to.add <- nrow(train) - 2 * length(minority.indices)
  # Duplicamos ejemplos aleatorios tantas veces como sea necesario.
  duplicate <- sample(minority.indices, to.add, replace = T)
  
  train <- rbind(train, train[duplicate,])
  classes.train <- c(classes.train, rep(0, to.add))
  
  # Probamos el conjunto de datos con el sobremuestreo.
  predictions <-  knn(train, test, classes.train, k = 3)
  knn.pred <- c(knn.pred, predictions)
}
tpr.ROS <- sum(circle$Class[as.vector(CVperm)] == 0 & knn.pred == 1) / nClass0
tnr.ROS <- sum(circle$Class[as.vector(CVperm)] == 1 & knn.pred == 3) / nClass1
gmean.ROS <- sqrt(tpr.ROS * tnr.ROS)
cat("\nTPR:", tpr.ROS, 
    "\nTNR:", tnr.ROS,
    "\nGmean:", gmean.ROS)
```

   Con este método pasamos a  $0.93$, con lo que podemos decir que este método es efectivo.
   
# Random Undersampling (RUS)
```{r}
knn.pred = NULL
for( i in 1:5){
  train <- circle[-CVperm[,i], -3]
  classes.train <- circle[-CVperm[,i], 3] 
  test  <- circle[CVperm[,i], -3]
  
  # randomly undersample the minority class (class 1)
  majority.indices <- which(classes.train == 1)
  to.remove <- 2* length(majority.indices) - nrow(train)[1]
  remove <- sample(majority.indices, to.remove, replace = F)
  train <- train[-remove,] 
  classes.train <- classes.train[-remove]
  
  # use the modified training set to make predictions
  predictions <-  knn(train, test, classes.train, k = 3)
  knn.pred <- c(knn.pred, predictions)
}
tpr.RUS <- sum(circle$Class[as.vector(CVperm)] == 0 & knn.pred == 1) / nClass0
tnr.RUS <- sum(circle$Class[as.vector(CVperm)] == 1 & knn.pred == 2) / nClass1
gmean.RUS <- sqrt(tpr.RUS * tnr.RUS)
cat("\nTPR:", tpr.RUS, 
    "\nTNR:", tnr.RUS,
    "\nGmean:", gmean.RUS)
```

   Con el bajomuestreo obtenemos una medida aún superior, pues tenemos un acierto de 1 para la clase positiva. En la Figura \ref{fig:circle.RUS} mostramos el conjunto de datos después de muestrear.

```{r circle.RUS, fig.cap=" Representación bajomuestreo"}
# Visualization (RUS on the full dataset)
circle.RUS <- circle
majority.indices <- (1:dim(circle.RUS)[1])[circle.RUS$Class == 1]
to.remove <- 2 * length(majority.indices) - dim(circle.RUS)[1]
remove <- sample(majority.indices, to.remove, replace = F)
circle.RUS <- circle.RUS[-remove,] 

ggplot(circle.RUS, aes(x=Att1, y = Att2, color = Class)) + geom_point() 
```


# SMOTE

## Distancia

```{r}
distance <- function(i, j, data){
   dist <- 0
   for(f in 1:ncol(data)){
      if(is.factor(data[ ,f])){ # nominal feature
         if(data[i,f] != data[j,f]){
            dist <- dist + 1
         }
      } 
      else {
         dist <- dist + (data[i,f] - data[j,f]) * (data[i,f] - data[j,f])
      }
   }
   dist <- sqrt(dist)
   return(dist)
}
```


## Obtener vecinos cercanos

```{r}
getNeighbors <- function(x, minority.instances, train, k = 5){
   
   distance.minority <- sapply(minority.instances, 
                               function(j) 
                                  distance(x, j, train))
   
   return(minority.instances[order(distance.minority)][1:k])
}
```

## Synthetic Instance

```{r}
syntheticInstance <- function(x, neighbors, data){
   sel.neighbor <- sample(neighbors, 1)
   random.value <- runif(1)
   
   sapply(1:ncol(data), function(f){
      if(is.factor(data[ ,f])){ # nominal feature
         return(sample(c(data[x ,f], data[sel.neighbor,f]), 1))
      } 
      else {
         return(data[x ,f] + random.value * (data[sel.neighbor ,f] - data[x,f]))
      }
   }) -> synthetic 
   return(synthetic)
}
```

## Implementación de SMOTE

```{r smote}
SMOTE <- function(data, classes){
   minority.indexes <- which(classes == 0)
   to.create <- sum(classes == 1) - sum(classes == 0)
   generators.indexes <- sample(minority.indexes, size = to.create, replace = T)
   
   
   synthetic.instances <- sapply(generators.indexes,
         function(x){
            neighbors <- getNeighbors(x, minority.indices, data)
            return(syntheticInstance(x, neighbors, data))
         })
   synthetic.instances <- t(synthetic.instances)
   colnames(synthetic.instances) <- colnames(data)
   return(rbind(data, synthetic.instances))
}
```


```{r}
knn.pred = NULL
for(i in 1:5){
  train <- circle[-CVperm[,i], -3]
  classes.train <- circle[-CVperm[,i], 3] 
  test  <- circle[CVperm[,i], -3]
  
  # Generate synthetic instances
  train <- SMOTE(train, classes.train) 
  classes.train <- c(classes.train, rep(0, sum(classes.train == 1) - sum(classes.train == 0)))
  
  # use the modified training set to make predictions
  predictions <-  knn(train, test, classes.train, k = 3)
  knn.pred <- c(knn.pred, predictions)
}
tpr.SMOTE <- sum(circle$Class[as.vector(CVperm)] == 0 & knn.pred == 1) / nClass0
tnr.SMOTE <- sum(circle$Class[as.vector(CVperm)] == 1 & knn.pred == 2) / nClass1
gmean.SMOTE <- sqrt(tpr.RUS * tnr.RUS)
cat("\nTPR:", tpr.RUS, 
    "\nTNR:", tnr.RUS,
    "\nGmean:", gmean.RUS)
```

   
   Con la técnica de SMOTE seguimos teniendo una alta tasa de acierto para la clase positiva y la medida $G$ es de $0.95$, con lo que para este conjunto no supone una mejor relevante SMOTE sobre RUS.

```{r circle.SMOTE, fig.cap=" Representación SMOTE"}
# Visualization (RUS on the full dataset)
circle.SMOTE <- SMOTE(circle[,-3],circle[,3])
circle.SMOTE$Class <- c(circle$Class,  rep(0, sum(circle$Class == 1) - sum(circle$Class == 0)))
ggplot(circle.SMOTE, aes(x=Att1, y = Att2, color = Class)) + geom_point() 
```