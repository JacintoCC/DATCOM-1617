#+TITLE: Práctica 4
#+SUBTITLE: Modelos gráficos probabilísticos
#+AUTHOR: Jacinto Carrasco Castillo


* Descripción del problema

El problema consiste en aprender clasificadores para el conjunto de
datos =ledLXMn30.arff=.

Este conjunto de datos representa una serie de instancias en las que
cada una de ellas contiene siete características que se corresponden
con el estado (binario) de siete segmentos de una pantalla LED y 17
otros atributos binarios que son irrelevantes para el problema. La
tarea consiste en predecir el dígito representado en la pantalla LED. 

El conjunto de datos ocntiene ruido, de manera que cada segmento LED
puede alternar su estado con un $30\%$ de probabilidad. Si abrimos el
fichero correspondiente en WEKA, observamos que el número de
instancias para cada clase está en torno a 1000, siendo el más
frecuente el =5= (1050 instancias) y el =0= el menos frecuente (913
instancias).  Podemos considerar entonces que no hay un desbalanceo
a tener en cuenta entre las clases y no hará falta preprocesar los
datos para lograr una mayor paridad entre las clases.

* Preprocesamiento de los datos 

Como se comenta en el guion de la práctica, hay 17 atributos que no
tienen una información relevante, algo que podemos comprobar con
únicamente observar la distribución de las clases por atributos. De la
variable 8 a la 24 vemos que prácticamente una mitad de las instancias
de cada clase van a parar a cada valor del atributo, es decir $P(C|A)
= P(C|¬A)$, donde $C$ es la clase y $A$ uno de los atributos
mencionados. 

Probamos por tanto a utilizar algún método de selección de
características, como por ejemplo usando la ganancia de información
para ordenar los atributos. De esta manera obtenemos que los atributos
del 8 al 24 son los que menos ganancia de información aporta y ésta es
menor o igual que 0.001, con lo que consideramos descartar estas
variables. 

Eliminar estas variables hace que mejore muy ligeramente el acierto
del clasificador Naïve Bayes usado como punto de partida, por lo que
consideraremos que este paso es acertado, pues, como mínimo, tenemos
menos variables y el modelo obtenido será más simple. 


* Redes con mejores tasas

** Resultado de partida

Antes de realizar ningún preprocesamiento, probamos a ejecutar el
modelo Naïve Bayes, el cual obtiene un acierto de 1/3 en la validación
cruzada.

** Naïve Bayes

Una vez que quitamos las variables artificialmente introducidas, el
clasificador =Naïve Bayes= obtiene un acierto del $33.41\%$ en
validación cruzada.

** Red Bayesiana

*** ICS

Con el algoritmo de búsqueda ICS el acierto baja al $33.37\%$. 

*** K2

Con la búsqueda local K2 el resultado es $33.36%$ en validación cruzada.

